{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# El proceso de clasificación lineal\n",
    "\n",
    "La clasificación es una subcategoría del aprendizaje supervisado donde el objetivo es\n",
    "predecir las etiquetas de clase de nuevas instancias basadas en observaciones pasadas (aprendizaje).\n",
    "Esas etiquetas de clase son valores discretos y desordenados que pueden entenderse como\n",
    "membresías grupales de las instancias.\n",
    "\n",
    "\n",
    "\n",
    "1. Clasificadores lineales.\n",
    "2. Métricas de clasificación.\n",
    "3. Máquinas de vectores de soporte (SVM)\n",
    "4. Clasificación multiclase en clasificadores binarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Clasificadores lineales\n",
    "\n",
    "### 1.1 Perceptrón\n",
    "\n",
    "Fué propuesto por F. Rosenblatt, 1958 - Basado en el modelo neuronal propuesto por McCulloch-Pitts. Es un algoritmo\n",
    "que **aprende de forma óptima los coeficientes de los pesos** de la neurona que una vez multiplicados por las características\n",
    "de entrada permite determinar si la neurona se activa o no.\n",
    "\n",
    "**Teoría básica**\n",
    "\n",
    "Este problema permite moldear una tarea de clasificación binaria donde tenemos 2 clases ( 1 y 0) en la que definimos\n",
    "una función de activación $\\phi(z)$ que se calcula como una combinación lineal de un vector de características\n",
    "$\\mathbf{x}$ y un vector de pesos $\\mathbf{w}$. Donde $z = w_1x_1 + \\ldots + w_mx_m$.\n",
    "\n",
    "Entonces, dado un ejemplo $x^{(i)}$, si la salida de $\\phi(z)$ es mayor que un determinado valor ( _threshold_ )\n",
    "$\\theta$ podemos predecir que este pertenece a la clase 1 y en de lo contrario diremos que pertenece a la clase -1.\n",
    "esto se conoce como función escalón (en inglés _step function_ ).\n",
    "\n",
    "$\\phi(z) = \\begin{cases}\n",
    " 1 & \\text{if z} >= 0, \\\\\n",
    " 0 & \\text{en caso contrario}\n",
    "\\end{cases}$\n",
    "\n",
    "Uso del perceptrón en scikit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "X,y = ... # nuestros datos\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(X, y)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Regresión Logística\n",
    "Aunque el método del perceptrón ofrece una introducción sencilla a los algoritmos de aprendizaje automático\n",
    "para la tarea de clasificación, su mayor desventaja es que nunca converge si las clases no son perfectamente linealmente\n",
    "separables.\n",
    "\n",
    "La regresión logística es un modelo de clasificación que sencillo de usar pero que funciona muy bien en clases\n",
    "linealmente separables. Es uno de los algoritmos de clasificación más utilizados en la industria. Hay que tener clara\n",
    "la idea que la regresión logística es un **modelo probabilístico**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X,y = ... # nuestros datos\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X, y)\n",
    "clf.predict(X) # predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Métricas\n",
    "Una vez que hemos entrenado nuestro modelo, tenemos la necesidad de evaluar los resultados obtenidos con alguna\n",
    "medida que sea objetiva. Las medidas que explicaremos en esta sección se calculan a partir de una matriz de\n",
    "confusión que nos permite guardar cuatro medidas básicas a partir de considerar que una de las clases es positiva\n",
    "y la otra es la negativa.\n",
    "\n",
    "- _True Positives_ (TP): El algoritmo clasifica una muestra de la clase positiva como miembro de la clase\n",
    "positiva.\n",
    "- _True Negatives_ (TN): El algoritmo clasifica una muestra de la clase negativa como miembro de la clase\n",
    "negativa.\n",
    "- _False Positives_ (FP): El algoritmo clasifica una muestra de la clase negativa como miembro de la clase\n",
    "positiva.\n",
    "- _False Negatives_ (FN): El algoritmo clasifica una muestra de la clase positiva como miembro de la clase\n",
    "negativa.\n",
    "\n",
    "Podemos observar la matriz de confusión en el siguiente esquema:\n",
    "\n",
    "![image](img/confusion_matrix.png \"fuente: Python Machine Learning\")\n",
    "\n",
    "Esta matriz se puede obtener de forma sencilla usando la función `confusion_matrix` de la librería [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn-metrics-confusion-matrix)\n",
    "y se puede visualizar con la función [ConfusionMatrixDisplay](https://scikit-learn\n",
    ".org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html?highlight=confusion%20matrix#sklearn-metrics-confusionmatrixdisplay)\n",
    "\n",
    "A partir de estas medidas de primer orden podemos obtener algunas otras más completas:\n",
    "\n",
    "$$ Error = \\frac{FP+FN}{FP+FN+TP+TN}$$\n",
    "<br>\n",
    "$$ Exactitud = \\frac{TP+TN}{FP+FN+TP+TN} = 1 - Error$$, también conocido como _Accuracy_.\n",
    "\n",
    "Por otro lado tenemos las medidas Ratio de Verdaderos Positivos (True Positive Rate, TPR) y el Ratio de Falsos Positivos\n",
    "(False Positive Rate, FPR) que están diseñadas para problemas en los que hay una clase con más muestras que la otra:\n",
    "\n",
    "$$ FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN} $$\n",
    "<br>\n",
    "$$ TPR = \\frac{TP}{P} = \\frac{TP}{FN+TP} $$\n",
    "\n",
    "Por último hablaremos de precisión (_precision_) y la sensibilidad (_recall_) relacionadas con los ratios de verdaderos\n",
    "positivos y verdaderos negativos:\n",
    "\n",
    "$$ Precisión = \\frac{TP}{TP+FP}$$\n",
    "<br>\n",
    "$$Sensibilidad = TPR = \\frac{TP}{FN+TP} $$\n",
    "\n",
    "Tenemos una medida que engloba estas dos medidas anteriores:\n",
    "\n",
    "$$ F1 = 2 \\frac{Precision \\times Sensibilidad}{Precision + Sensibiliad}$$\n",
    "\n",
    "Por suerte, en scikit-learn tenemos un módulo llamado _metrics_ donde están todas estas (y otras) métricas ya\n",
    "implementadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Máquinas de vectores de soporte (SVM)\n",
    "\n",
    "Esta es una de las técnicas más usadas en la actualidad. A diferencia del perceptron donde intentamos minimizar las\n",
    "clasificaciones erróneas, en esta caso el objetivo de optimización es maximizar el margen entre las clases. El margen\n",
    "se define como la distancia entre el hiperplano de separación (límite de decisión) y las muestras de entrenamiento\n",
    "más cercanas a este hiperplano, que son los llamados **vectores de soporte**.\n",
    "\n",
    "\n",
    "Este método tiene un parámetro, `C`, que sirve como valor de regularización. Este parámetro es útil para permitir que el algoritmo pueda converger en situaciones de errores en la clasificación cuando el conjunto de datos no es\n",
    "linealmente separable. Usando el hiperparámetro `C` podemos controlar la penalización por realizar una clasificación errónea. \n",
    "\n",
    "- Los **valores grandes** de `C` corresponden a penalizaciones de error grandes. \n",
    "\n",
    "- Si elegimos **valores más pequeños** para `C` somos menos estrictos con los errores de clasificación errónea. \n",
    "\n",
    "Podemos usar el parámetro`C` para controlar el ancho del margen de clasificación y, por lo tanto, ajustar la compensación de sesgo-varianza.\n",
    "\n",
    "Uso en scikit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X, y = ... # Nuestros datos\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "clf = LinearSVC(C=1.0, random_state=0)\n",
    "clf.fit(X_scaled, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clasificación multiclase\n",
    "\n",
    "Los algoritmos que hemos trabajado en esta sesión, están diseñados para problemas de clasificación binaria. Como tales, no se pueden utilizar para tareas de clasificación de múltiples clases, al menos no directamente.\n",
    "\n",
    "En su lugar, se pueden utilizar métodos heurísticos para dividir un problema de clasificación de múltiples clases en varios conjuntos de datos de clasificación binaria y entrenar un modelo de clasificación binaria para cada uno.\n",
    "\n",
    "En particular en scikit siempre tendremos disponible la opción OVR:\n",
    "\n",
    "### One-vs-rest\n",
    "\n",
    "Este método implica dividir el conjunto de datos de múltiples clases en varios problemas de clasificación binaria.\n",
    "Luego, se entrena un clasificador binario para cada uno de los problemas de clasificación binaria creados y se hacen\n",
    "predicciones utilizando el modelo más confiable.\n",
    "\n",
    "Por ejemplo, dado un problema de clasificación con múltiples clases con ejemplos para cada clase \"rojo\", \"azul\" y\n",
    "\"verde\". Esto podría dividirse en tres conjuntos de datos de clasificación binaria de la siguiente manera:\n",
    "\n",
    "- Problema de clasificación binaria 1: rojo frente a [azul, verde]\n",
    "- Problema de clasificación binaria 2: azul frente a [rojo, verde]\n",
    "- Problema de clasificación binaria 3: verde frente a [rojo, azul]\n",
    "\n",
    "Una posible desventaja de este enfoque es que requiere la creación de un modelo para cada clase. Por ejemplo, tres\n",
    "clases requieren tres modelos. Esto podría ser un problema para grandes conjuntos de datos, modelos que requieren\n",
    "grandes tiempos para entrenar o que tienen un gran número de clases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
